{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-18T10:42:25.317843Z","iopub.execute_input":"2022-09-18T10:42:25.318678Z","iopub.status.idle":"2022-09-18T10:42:25.327767Z","shell.execute_reply.started":"2022-09-18T10:42:25.318640Z","shell.execute_reply":"2022-09-18T10:42:25.326265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading our training data\nimport pandas as pd\nimport numpy as np\n\norg_train_data = pd.read_csv('../input/titanic/train.csv')\ntrain_data = org_train_data.copy()\ntrain_data = train_data.drop(['PassengerId'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:27.528478Z","iopub.execute_input":"2022-09-18T10:42:27.528877Z","iopub.status.idle":"2022-09-18T10:42:27.542602Z","shell.execute_reply.started":"2022-09-18T10:42:27.528845Z","shell.execute_reply":"2022-09-18T10:42:27.541406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ordinal encoding categorical data (all obj dtypes)\ntrain_data['Sex'] = pd.factorize(train_data['Sex'])[0]\ntrain_data['Ticket'] = pd.factorize(train_data['Ticket'])[0]\ntrain_data['Cabin'] = pd.factorize(train_data['Cabin'])[0]\ntrain_data['Embarked'] = pd.factorize(train_data['Embarked'])[0]\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:28.478327Z","iopub.execute_input":"2022-09-18T10:42:28.479306Z","iopub.status.idle":"2022-09-18T10:42:28.507138Z","shell.execute_reply.started":"2022-09-18T10:42:28.479263Z","shell.execute_reply":"2022-09-18T10:42:28.505817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ordinal encoding name column (on a passenger status basis)\nnew_name_list = []\nname_list = train_data['Name']\n\nfor x in name_list:\n    if 'Mr.' in x:\n        new_name_list.append('1')\n    elif 'Mrs.' in x:\n        new_name_list.append('2')\n    elif 'Miss.' in x:\n        new_name_list.append('3')\n    elif 'Rev.' in x:\n        new_name_list.append('4')\n    elif 'Master.' in x:\n        new_name_list.append('5')\n    elif 'Dr.' in x:\n        new_name_list.append('6')\n    elif 'Don.' in x:\n        new_name_list.append('7')\n    else:\n        new_name_list.append('0')\n\ntrain_data['Name'] = new_name_list\ntrain_data['Name'] = train_data['Name'].astype(int)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:31.126372Z","iopub.execute_input":"2022-09-18T10:42:31.127258Z","iopub.status.idle":"2022-09-18T10:42:31.137466Z","shell.execute_reply.started":"2022-09-18T10:42:31.127218Z","shell.execute_reply":"2022-09-18T10:42:31.136258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balancing our training data set (for there are far fewer survivors)\n\nsurvived_list = train_data[train_data['Survived'] == 1]\ndead_list = train_data[train_data['Survived'] == 0]\n\nnew_dead_list = dead_list.iloc[:len(survived_list),:]\nbalanced_train_data = pd.concat([survived_list,new_dead_list])\nbalanced_train_data\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:33.164732Z","iopub.execute_input":"2022-09-18T10:42:33.165189Z","iopub.status.idle":"2022-09-18T10:42:33.190912Z","shell.execute_reply.started":"2022-09-18T10:42:33.165152Z","shell.execute_reply":"2022-09-18T10:42:33.189664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dealing with missing values- imputing and extending our dataset with a column indicating wether a value was missing\nfrom sklearn.impute import SimpleImputer\n\ncols_w_missing = [col for col in balanced_train_data.columns\n                 if balanced_train_data[col].isnull().any()]\nfor col in cols_w_missing:\n    balanced_train_data[col + \"_was_missing\"] = balanced_train_data[col].isnull()\n\nimputer = SimpleImputer()\nimputed_data = pd.DataFrame(imputer.fit_transform(balanced_train_data))\nimputed_data.columns = balanced_train_data.columns\nimputed_data = imputed_data.astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:35.954805Z","iopub.execute_input":"2022-09-18T10:42:35.955205Z","iopub.status.idle":"2022-09-18T10:42:35.972996Z","shell.execute_reply.started":"2022-09-18T10:42:35.955173Z","shell.execute_reply":"2022-09-18T10:42:35.972117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engeneering- Mutual Information\n# After labeling all categorical data, we will look for the contribution to uncertainty reduction for our target column using the MI regression tool.\n# That way we'd be able to select the best features to use for our clasification models.\n\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\n\ncomparison_feature = imputed_data.Survived\ncompared_features = imputed_data.drop(['Survived'], axis=1)\n\n# Dealing with discrete values using mi reg\ndiscrete_features = compared_features.dtypes == int\n\ndef mi_score(X,y,discrete_features):\n    mi_scores = mutual_info_regression(X,y,discrete_features = discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = mi_score(compared_features,comparison_feature,discrete_features)\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"MI Scores\")\nplot_mi_scores(mi_scores)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:38.213003Z","iopub.execute_input":"2022-09-18T10:42:38.213427Z","iopub.status.idle":"2022-09-18T10:42:38.605428Z","shell.execute_reply.started":"2022-09-18T10:42:38.213393Z","shell.execute_reply":"2022-09-18T10:42:38.604101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"\n\n","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# After examining the results, we can get rid of the last 3 features for they are pretty much inefficient\n\nimputed_data = imputed_data.drop(['Age_was_missing','SibSp','Parch'], axis=1)\nimputed_data","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:41.911537Z","iopub.execute_input":"2022-09-18T10:42:41.912226Z","iopub.status.idle":"2022-09-18T10:42:41.927510Z","shell.execute_reply.started":"2022-09-18T10:42:41.912189Z","shell.execute_reply":"2022-09-18T10:42:41.926673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-Hot encoding for Name, Embarked columns\nfrom sklearn.model_selection import train_test_split\n\n\n\nname_train_dummies = pd.get_dummies(data=imputed_data['Name'])\nemb_train_dummies = pd.get_dummies(data=imputed_data['Embarked'])\n\n\nimputed_data = imputed_data.drop([\"Name\",\"Embarked\"], axis=1)\n\n\nimputed_data = pd.concat([imputed_data, name_train_dummies, emb_train_dummies], axis=1)\n\n\nimputed_data = imputed_data.rename(columns = {'0':'NoStatus', '1':'Mr', '2':'Mrs', '3':'Miss', '4':'Rev', '5':'Master'\n                                    , '6':'Dr', '7':'Don'})\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:44.618819Z","iopub.execute_input":"2022-09-18T10:42:44.619249Z","iopub.status.idle":"2022-09-18T10:42:44.632208Z","shell.execute_reply.started":"2022-09-18T10:42:44.619212Z","shell.execute_reply":"2022-09-18T10:42:44.630578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = imputed_data.Survived\nX = imputed_data.drop(['Survived'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:42:47.547200Z","iopub.execute_input":"2022-09-18T10:42:47.548116Z","iopub.status.idle":"2022-09-18T10:42:47.556749Z","shell.execute_reply.started":"2022-09-18T10:42:47.548077Z","shell.execute_reply":"2022-09-18T10:42:47.555630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model & Training - Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Choosing the optimal n_estimator value\nscores = []\nfor k in range(1,200):\n    rand_forest = RandomForestClassifier(n_estimators = k)\n    rand_forest.fit(X_train, y_train)\n    preds = rand_forest.predict(X_valid)\n    scores.append(accuracy_score(y_valid, preds))\n    \nplt.plot(range(1,200),scores)\nplt.xlable('n_estimators')\nplt.ylabel('Prediction accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:45:16.441647Z","iopub.execute_input":"2022-09-18T10:45:16.442052Z","iopub.status.idle":"2022-09-18T10:45:57.142789Z","shell.execute_reply.started":"2022-09-18T10:45:16.442020Z","shell.execute_reply":"2022-09-18T10:45:57.141072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we can see- the optimal value is approximately 15, hence:\n\nrand_forest = RandomForestClassifier(n_estimators = 15)\nrand_forest.fit(X_train, y_train)\npreds = rand_forest.predict(X_valid)\nscore = accuracy_score(y_valid, preds)","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:10.546984Z","iopub.execute_input":"2022-09-18T10:47:10.547819Z","iopub.status.idle":"2022-09-18T10:47:10.592963Z","shell.execute_reply.started":"2022-09-18T10:47:10.547762Z","shell.execute_reply":"2022-09-18T10:47:10.591593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_list = {'Random Forest Classifier':[score]}\nscore_matrix = pd.DataFrame(score_list)\nscore_matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:13.073175Z","iopub.execute_input":"2022-09-18T10:47:13.074300Z","iopub.status.idle":"2022-09-18T10:47:13.085026Z","shell.execute_reply.started":"2022-09-18T10:47:13.074241Z","shell.execute_reply":"2022-09-18T10:47:13.083812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model & Training - Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ngnb_preds = gnb.predict(X_valid)\nnb = [accuracy_score(y_valid, gnb_preds)]\nscore_matrix['NaiveBayes'] = nb","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:26.976521Z","iopub.execute_input":"2022-09-18T10:47:26.976936Z","iopub.status.idle":"2022-09-18T10:47:26.991733Z","shell.execute_reply.started":"2022-09-18T10:47:26.976906Z","shell.execute_reply":"2022-09-18T10:47:26.990450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model & Training - Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg_preds = logreg.predict(X_valid)\nlr = [accuracy_score(y_valid, logreg_preds)]\nscore_matrix['LogisticREgression'] = lr\nscore_matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:30.549271Z","iopub.execute_input":"2022-09-18T10:47:30.549679Z","iopub.status.idle":"2022-09-18T10:47:30.651021Z","shell.execute_reply.started":"2022-09-18T10:47:30.549647Z","shell.execute_reply":"2022-09-18T10:47:30.649413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model & Training - K Nearest Neighbor\n# Finding the optimal value for k\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscores = []\nfor k in range(1,200):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    knn_preds = knn.predict(X_valid)\n    scores.append(accuracy_score(y_valid, knn_preds))\n\nplt.plot(range(1,200), scores)\nplt.xlabel('K neighbors')\nplt.ylabel('Predictions accuracy')\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:34.979926Z","iopub.execute_input":"2022-09-18T10:47:34.980366Z","iopub.status.idle":"2022-09-18T10:47:40.404475Z","shell.execute_reply.started":"2022-09-18T10:47:34.980330Z","shell.execute_reply":"2022-09-18T10:47:40.403301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we can see the optimal value is around 15, hence:\n\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X_train, y_train)\nknn_preds = knn.predict(X_valid)\nknn_mat = accuracy_score(y_valid, knn_preds)\nscore_matrix['KNN'] = knn_mat\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:49.245467Z","iopub.execute_input":"2022-09-18T10:47:49.246731Z","iopub.status.idle":"2022-09-18T10:47:49.275203Z","shell.execute_reply.started":"2022-09-18T10:47:49.246678Z","shell.execute_reply":"2022-09-18T10:47:49.273728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model & Training - Support Vector Machine\nfrom sklearn import svm\nsvm = svm.SVC()\nsvm.fit(X_train, y_train)\nsvm_preds = svm.predict(X_valid)\nsvm_mat = accuracy_score(y_valid, svm_preds)\nscore_matrix['SVM'] = svm_mat\n","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:52.292046Z","iopub.execute_input":"2022-09-18T10:47:52.292484Z","iopub.status.idle":"2022-09-18T10:47:52.321834Z","shell.execute_reply.started":"2022-09-18T10:47:52.292449Z","shell.execute_reply":"2022-09-18T10:47:52.320757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_matrix","metadata":{"execution":{"iopub.status.busy":"2022-09-18T10:47:55.621089Z","iopub.execute_input":"2022-09-18T10:47:55.623729Z","iopub.status.idle":"2022-09-18T10:47:55.638808Z","shell.execute_reply.started":"2022-09-18T10:47:55.623671Z","shell.execute_reply":"2022-09-18T10:47:55.637373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}